
@article{summervilleSuperMarioString2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.00930},
  primaryClass = {cs},
  title = {Super {{Mario}} as a {{String}}: {{Platformer Level Generation Via LSTMs}}},
  url = {http://arxiv.org/abs/1603.00930},
  shorttitle = {Super {{Mario}} as a {{String}}},
  abstract = {The procedural generation of video game levels has existed for at least 30 years, but only recently have machine learning approaches been used to generate levels without specifying the rules for generation. A number of these have looked at platformer levels as a sequence of characters and performed generation using Markov chains. In this paper we examine the use of Long Short-Term Memory recurrent neural networks (LSTMs) for the purpose of generating levels trained from a corpus of Super Mario Brothers levels. We analyze a number of different data representations and how the generated levels fit into the space of human authored Super Mario Brothers levels.},
  urldate = {2019-09-11},
  date = {2016-03-02},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Summerville, Adam and Mateas, Michael},
  file = {/home/jan/Zotero/storage/FYTADB54/Summerville and Mateas - 2016 - Super Mario as a String Platformer Level Generati.pdf;/home/jan/Zotero/storage/33PQ4DLD/1603.html}
}

@article{giacomelloDOOMLevelGeneration2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.09154},
  primaryClass = {cs, stat},
  title = {{{DOOM Level Generation}} Using {{Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1804.09154},
  abstract = {We applied Generative Adversarial Networks (GANs) to learn a model of DOOM levels from human-designed content. Initially, we analysed the levels and extracted several topological features. Then, for each level, we extracted a set of images identifying the occupied area, the height map, the walls, and the position of game objects. We trained two GANs: one using plain level images, one using both the images and some of the features extracted during the preliminary analysis. We used the two networks to generate new levels and compared the results to assess whether the network trained using also the topological features could generate levels more similar to human-designed ones. Our results show that GANs can capture intrinsic structure of DOOM levels and appears to be a promising approach to level generation in first person shooter games.},
  urldate = {2019-09-11},
  date = {2018-04-24},
  keywords = {Computer Science - Machine Learning,Computer Science - Human-Computer Interaction,Statistics - Machine Learning},
  author = {Giacomello, Edoardo and Lanzi, Pier Luca and Loiacono, Daniele},
  file = {/home/jan/Zotero/storage/2FEN52F4/Giacomello et al. - 2018 - DOOM Level Generation using Generative Adversarial.pdf;/home/jan/Zotero/storage/U6ZH4CB9/1804.html}
}

@article{stephensonAgentBasedAdaptiveLevel2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.02518},
  primaryClass = {cs},
  title = {Agent-{{Based Adaptive Level Generation}} for {{Dynamic Difficulty Adjustment}} in {{Angry Birds}}},
  url = {http://arxiv.org/abs/1902.02518},
  abstract = {This paper presents an adaptive level generation algorithm for the physics-based puzzle game Angry Birds. The proposed algorithm is based on a pre-existing level generator for this game, but where the difficulty of the generated levels can be adjusted based on the player's performance. This allows for the creation of personalised levels tailored specifically to the player's own abilities. The effectiveness of our proposed method is evaluated using several agents with differing strategies and AI techniques. By using these agents as models / representations of real human player's characteristics, we can optimise level properties efficiently over a large number of generations. As a secondary investigation, we also demonstrate that by combining the performance of several agents together it is possible to generate levels that are especially challenging for certain players but not others.},
  urldate = {2019-09-11},
  date = {2019-02-07},
  keywords = {Computer Science - Artificial Intelligence},
  author = {Stephenson, Matthew and Renz, Jochen},
  file = {/home/jan/Zotero/storage/LLAFC2W8/Stephenson and Renz - 2019 - Agent-Based Adaptive Level Generation for Dynamic .pdf;/home/jan/Zotero/storage/FNJY25YB/1902.html}
}

@article{ashlockAutomaticGenerationLevel2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.09618},
  primaryClass = {cs},
  title = {Automatic {{Generation}} of {{Level Maps}} with the {{Do What}}'s {{Possible Representation}}},
  url = {http://arxiv.org/abs/1905.09618},
  abstract = {Automatic generation of level maps is a popular form of automatic content generation. In this study, a recently developed technique employing the \{\textbackslash{}em do what's possible\} representation is used to create open-ended level maps. Generation of the map can continue indefinitely, yielding a highly scalable representation. A parameter study is performed to find good parameters for the evolutionary algorithm used to locate high-quality map generators. Variations on the technique are presented, demonstrating its versatility, and an algorithmic variant is given that both improves performance and changes the character of maps located. The ability of the map to adapt to different regions where the map is permitted to occupy space are also tested.},
  urldate = {2019-09-11},
  date = {2019-05-23},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  author = {Ashlock, Daniel and Salge, Christoph},
  file = {/home/jan/Zotero/storage/NLV87BLS/Ashlock and Salge - 2019 - Automatic Generation of Level Maps with the Do Wha.pdf;/home/jan/Zotero/storage/GM6M8U4U/1905.html}
}

@online{GameLevelGeneration,
  langid = {english},
  title = {Game {{Level Generation Using Neural Networks}}},
  url = {https://www.gamasutra.com/blogs/SeungbackShin/20180227/315017/Game_Level_Generation_Using_Neural_Networks.php},
  abstract = {We discuss here how we generated game levels using artificial neural network in our game Fantasy Raiders. In addition, we suggested you to consider how the machine learning can provide new perspectives and inspirations to the game designers.},
  urldate = {2019-09-11},
  file = {/home/jan/Zotero/storage/C26BC3EH/Game_Level_Generation_Using_Neural_Networks.html}
}

@article{summervilleProceduralContentGeneration2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.00539},
  primaryClass = {cs},
  title = {Procedural {{Content Generation}} via {{Machine Learning}} ({{PCGML}})},
  url = {http://arxiv.org/abs/1702.00539},
  abstract = {This survey explores Procedural Content Generation via Machine Learning (PCGML), defined as the generation of game content using machine learning models trained on existing content. As the importance of PCG for game development increases, researchers explore new avenues for generating high-quality content with or without human involvement; this paper addresses the relatively new paradigm of using machine learning (in contrast with search-based, solver-based, and constructive methods). We focus on what is most often considered functional game content such as platformer levels, game maps, interactive fiction stories, and cards in collectible card games, as opposed to cosmetic content such as sprites and sound effects. In addition to using PCG for autonomous generation, co-creativity, mixed-initiative design, and compression, PCGML is suited for repair, critique, and content analysis because of its focus on modeling existing content. We discuss various data sources and representations that affect the resulting generated content. Multiple PCGML methods are covered, including neural networks, long short-term memory (LSTM) networks, autoencoders, and deep convolutional networks; Markov models, \$n\$-grams, and multi-dimensional Markov chains; clustering; and matrix factorization. Finally, we discuss open problems in the application of PCGML, including learning from small datasets, lack of training data, multi-layered learning, style-transfer, parameter tuning, and PCG as a game mechanic.},
  urldate = {2019-09-11},
  date = {2017-02-01},
  keywords = {Computer Science - Artificial Intelligence},
  author = {Summerville, Adam and Snodgrass, Sam and Guzdial, Matthew and Holmg√•rd, Christoffer and Hoover, Amy K. and Isaksen, Aaron and Nealen, Andy and Togelius, Julian},
  file = {/home/jan/Zotero/storage/XA9QZMWR/Summerville et al. - 2017 - Procedural Content Generation via Machine Learning.pdf;/home/jan/Zotero/storage/EIUDIRFC/1702.html}
}

@article{vaswaniAttentionAllYou2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03762},
  primaryClass = {cs},
  title = {Attention {{Is All You Need}}},
  url = {http://arxiv.org/abs/1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  urldate = {2019-09-11},
  date = {2017-06-12},
  keywords = {Computer Science - Machine Learning,Computer Science - Computation and Language},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  file = {/home/jan/Zotero/storage/K3XNKZUX/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/jan/Zotero/storage/TJFHJM5Y/1706.html}
}

@article{yannakakisArtificialIntelligenceGames,
  langid = {english},
  title = {Artificial {{Intelligence}} and {{Games}}},
  pages = {359},
  author = {Yannakakis, Georgios N and Togelius, Julian},
  file = {/home/jan/Zotero/storage/2RD6NNPS/Yannakakis and Togelius - ArtiÔ¨Åcial Intelligence and Games.pdf}
}

@online{ArtificialIntelligenceGames,
  langid = {american},
  title = {Artificial {{Intelligence}} and {{Games}} ‚Äì {{A Springer Textbook}} | {{By Georgios N}}. {{Yannakakis}} and {{Julian Togelius}}},
  url = {http://gameaibook.org/},
  urldate = {2019-09-11},
  file = {/home/jan/Zotero/storage/BMBVAPAA/gameaibook.org.html}
}

@software{guminMxgmnWaveFunctionCollapse2019,
  title = {Mxgmn/{{WaveFunctionCollapse}}},
  url = {https://github.com/mxgmn/WaveFunctionCollapse},
  abstract = {Bitmap \& tilemap generation from a single example with the help of ideas from quantum mechanics.},
  urldate = {2019-09-11},
  date = {2019-09-11T11:14:01Z},
  keywords = {algorithm,csharp,gamedev,machine-learning,procedural-generation,wfc},
  author = {Gumin, Maxim},
  origdate = {2016-09-30T11:53:17Z}
}

@software{thehedgeifyTheHedgeifyDagstuhlGAN2019,
  title = {{{TheHedgeify}}/{{DagstuhlGAN}}},
  url = {https://github.com/TheHedgeify/DagstuhlGAN},
  abstract = {Contribute to TheHedgeify/DagstuhlGAN development by creating an account on GitHub.},
  urldate = {2019-09-11},
  date = {2019-08-26T15:50:24Z},
  author = {TheHedgeify},
  origdate = {2017-11-23T09:58:02Z}
}

@online{TheHedgeifyDagstuhlGAN,
  langid = {english},
  title = {{{TheHedgeify}}/{{DagstuhlGAN}}},
  url = {https://github.com/TheHedgeify/DagstuhlGAN},
  abstract = {Contribute to TheHedgeify/DagstuhlGAN development by creating an account on GitHub.},
  journaltitle = {GitHub},
  urldate = {2019-09-11},
  file = {/home/jan/Zotero/storage/GX5WFFAY/80c3913ef6f8afe07e6c45d4e5843e0336173523.html}
}

@online{MxgmnWaveFunctionCollapse,
  langid = {english},
  title = {Mxgmn/{{WaveFunctionCollapse}}},
  url = {https://github.com/mxgmn/WaveFunctionCollapse},
  abstract = {Bitmap \& tilemap generation from a single example with the help of ideas from quantum mechanics. - mxgmn/WaveFunctionCollapse},
  journaltitle = {GitHub},
  urldate = {2019-09-11},
  file = {/home/jan/Zotero/storage/ZQ7PL6YU/3157ed05751f7a25a78b623d3a5fe67488fc4ce7.html}
}

@article{volzEvolvingMarioLevels2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.00728},
  primaryClass = {cs},
  title = {Evolving {{Mario Levels}} in the {{Latent Space}} of a {{Deep Convolutional Generative Adversarial Network}}},
  url = {http://arxiv.org/abs/1805.00728},
  abstract = {Generative Adversarial Networks (GANs) are a machine learning approach capable of generating novel example outputs across a space of provided training examples. Procedural Content Generation (PCG) of levels for video games could benefit from such models, especially for games where there is a pre-existing corpus of levels to emulate. This paper trains a GAN to generate levels for Super Mario Bros using a level from the Video Game Level Corpus. The approach successfully generates a variety of levels similar to one in the original corpus, but is further improved by application of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Specifically, various fitness functions are used to discover levels within the latent space of the GAN that maximize desired properties. Simple static properties are optimized, such as a given distribution of tile types. Additionally, the champion A* agent from the 2009 Mario AI competition is used to assess whether a level is playable, and how many jumping actions are required to beat it. These fitness functions allow for the discovery of levels that exist within the space of examples designed by experts, and also guide the search towards levels that fulfill one or more specified objectives.},
  urldate = {2019-09-11},
  date = {2018-05-02},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  author = {Volz, Vanessa and Schrum, Jacob and Liu, Jialin and Lucas, Simon M. and Smith, Adam and Risi, Sebastian},
  file = {/home/jan/Zotero/storage/48PZYB3D/Volz et al. - 2018 - Evolving Mario Levels in the Latent Space of a Dee.pdf;/home/jan/Zotero/storage/DIJ6PKIS/1805.html}
}

@online{geitgeyMachineLearningFun2018,
  langid = {english},
  title = {Machine {{Learning}} Is {{Fun}}! {{Part}} 2},
  url = {https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3},
  abstract = {Using Machine Learning to generate Super Mario Maker levels},
  journaltitle = {Medium},
  urldate = {2019-09-11},
  date = {2018-11-07T22:26:02.587Z},
  author = {Geitgey, Adam},
  file = {/home/jan/Zotero/storage/M93K7F3L/machine-learning-is-fun-part-2-a26a10b68df3.html}
}

@video{BenBermanMachine2017,
  title = {Ben {{Berman}} - {{Machine Learning}} and {{Level Generation}}},
  url = {https://www.youtube.com/watch?v=Z6lHExfem6U},
  abstract = {Ben Berman talks about level generation using machine learning.
This video is from the 2017 Roguelike Celebration: https://roguelike.club/event2017.html},
  urldate = {2019-09-11},
  date = {2017-12-23}
}

@inproceedings{dahlskogLinearLevelsNgrams2014,
  langid = {english},
  location = {{Tampere, Finland}},
  title = {Linear Levels through N-Grams},
  isbn = {978-1-4503-3006-0},
  url = {http://dl.acm.org/citation.cfm?doid=2676467.2676506},
  doi = {10.1145/2676467.2676506},
  abstract = {We show that novel, linear game levels can be created using ngrams that have been trained on a corpus of existing levels. The method is fast and simple, and produces levels that are recognisably in the same style as those in the corpus that it has been trained on. We use Super Mario Bros. as an example domain, and use a selection of the levels from the original game as a training corpus. We treat Mario levels as a left-to-right sequence of vertical level slices, allowing us to perform level generation in a setting with some formal similarities to n-gram-based text generation and music generation. In empirical results, we investigate the effects of corpus size and n (sequence length). While the applicability of the method might seem limited to the relatively narrow domain of 2D games, we argue that many games in effect have linear levels and n-grams could be used to good effect, given that a suitable alphabet can be found.},
  eventtitle = {The 18th {{International Academic MindTrek Conference}}},
  booktitle = {Proceedings of the 18th {{International Academic MindTrek Conference}} on {{Media Business}}, {{Management}}, {{Content}} \& {{Services}} - {{AcademicMindTrek}} '14},
  publisher = {{ACM Press}},
  urldate = {2019-09-11},
  date = {2014},
  pages = {200-206},
  author = {Dahlskog, Steve and Togelius, Julian and Nelson, Mark J.},
  file = {/home/jan/Zotero/storage/VMEDIDF4/Dahlskog et al. - 2014 - Linear levels through n-grams.pdf}
}

@article{hooverComposingVideoGame,
  langid = {english},
  title = {Composing {{Video Game Levels}} with {{Music Metaphors}} through {{Functional Scaffolding}}},
  abstract = {Artists and other creators naturally draw inspiration for new works based on previous artifacts in their own fields. Some of the most profound examples of creativity, however, also transform the field by redefining and combining rules from other domains. In procedural content generation for games, representations and constraints are typically modeled on the target domain. In contrast, this paper examines representing and generating video game levels through a representation called functional scaffolding for musical composition originally designed to procedurally compose music. Viewing music as a means to re-frame the way we think about, represent and computationally design video game levels, this paper presents a method for deconstructing game levels into multiple ‚Äúinstruments‚Äù or ‚Äúvoices,‚Äù wherein each voice represents a tile type. We then use functional scaffolding to automatically generate ‚Äúaccompaniment‚Äù to individual voices. Complete new levels are subsequently synthesized from generated voices. Our proof-of-concept experiments showcase that music is a rich metaphor for representing naturalistic, yet unconventional and playable, levels in the classic platform game Super Mario Bros, demonstrating the capacity of our approach for potential applications in computational creativity and game design.},
  pages = {7},
  author = {Hoover, Amy K and Togelius, Julian and Yannakis, Georgios N},
  file = {/home/jan/Zotero/storage/PBIDZ6F8/Hoover et al. - Composing Video Game Levels with Music Metaphors t.pdf}
}

@incollection{fontConstrainedLevelGeneration2016,
  langid = {english},
  location = {{Cham}},
  title = {Constrained {{Level Generation Through Grammar}}-{{Based Evolutionary Algorithms}}},
  volume = {9597},
  isbn = {978-3-319-31203-3 978-3-319-31204-0},
  url = {http://link.springer.com/10.1007/978-3-319-31204-0_36},
  abstract = {This paper introduces an evolutionary method for generating levels for adventure games, combining speed, guaranteed solvability of levels and authorial control. For this purpose, a new graph-based two-phase level encoding scheme is developed. This method encodes the structure of the level as well as its contents into two abstraction layers: the higher level defines an abstract representation of the game level and the distribution of its content among different inter-connected game zones. The lower level describes the content of each game zone as a set of graphs containing rooms, doors, monsters, keys and treasure chests. Using this representation, game worlds are encoded as individuals in an evolutionary algorithm and evolved according to an evaluation function meant to approximate the entertainment provided by the game level. The algorithm is implemented into a design tool that can be used by game designers to specify several constraints of the worlds to be generated. This tool could be used to facilitate the design of game levels, for example to make professional-level content production possible for non-experts.},
  booktitle = {Applications of {{Evolutionary Computation}}},
  publisher = {{Springer International Publishing}},
  urldate = {2019-09-11},
  date = {2016},
  pages = {558-573},
  author = {Font, Jose M. and Izquierdo, Roberto and Manrique, Daniel and Togelius, Julian},
  editor = {Squillero, Giovanni and Burelli, Paolo},
  file = {/home/jan/Zotero/storage/5YDKDE2A/Font et al. - 2016 - Constrained Level Generation Through Grammar-Based.pdf},
  doi = {10.1007/978-3-319-31204-0_36}
}

@inproceedings{khalifaGeneralVideoGame2016,
  langid = {english},
  location = {{Denver, Colorado, USA}},
  title = {General {{Video Game Level Generation}}},
  isbn = {978-1-4503-4206-3},
  url = {http://dl.acm.org/citation.cfm?doid=2908812.2908920},
  doi = {10.1145/2908812.2908920},
  abstract = {This paper presents a framework and an initial study in general video game level generation, the problem of generating levels for not only a single game but for any game within a specified domain. While existing level generators are tailored to a particular game, this new challenge requires generators to take into account the constraints and affordances of games that might not even have been designed when the generator was constructed. The framework presented here builds on the General Video Game AI framework (GVG-AI) and the Video Game Description Language (VGDL), in order to reap synergies from research activities connected to the General Video Game Playing Competition. The framework will also form the basis for a new track of this competition. In addition to the framework, the paper presents three general level generators and an empirical comparison of their qualities.},
  eventtitle = {The 2016},
  booktitle = {Proceedings of the 2016 on {{Genetic}} and {{Evolutionary Computation Conference}} - {{GECCO}} '16},
  publisher = {{ACM Press}},
  urldate = {2019-09-11},
  date = {2016},
  pages = {253-259},
  author = {Khalifa, Ahmed and Perez-Liebana, Diego and Lucas, Simon M. and Togelius, Julian},
  file = {/home/jan/Zotero/storage/Z72TLRZA/Khalifa et al. - 2016 - General Video Game Level Generation.pdf}
}

@inreference{HistoryGames2019,
  langid = {english},
  title = {History of Games},
  url = {https://en.wikipedia.org/w/index.php?title=History_of_games&oldid=917083876},
  abstract = {The history of games dates to the ancient human past. Games are an integral part of all cultures and are one of the oldest forms of human social interaction. Games are formalized expressions of play which allow people to go beyond immediate imagination and direct physical activity. Common features of games include uncertainty of outcome, agreed upon rules, competition, separate place and time, elements of fiction, elements of chance, prescribed goals and personal enjoyment.
Games capture the ideas and worldviews of their cultures and pass them on to the future generation. Games were important as cultural and social bonding events, as teaching tools and as markers of social status. As pastimes of royalty and the elite, some games became common features of court culture and were also given as gifts. Games such as Senet and the Mesoamerican ball game were often imbued with mythic and ritual religious significance. Games like Gyan chauper and The Mansion of Happiness were used to teach spiritual and ethical lessons while Shatranj and W√©iq√≠ (Go) were seen as a way to develop strategic thinking and mental skill by the political and military elite.
In his 1938 book, Homo Ludens, Dutch cultural historian Johan Huizinga argued that games were a primary condition of the generation of human cultures. Huizinga saw the playing of games as something that ‚Äúis older than culture, for culture, however inadequately defined, always presupposes human society, and animals have not waited for man to teach them their playing.‚Äù Huizinga saw games as a starting point for complex human activities such as language, law, war, philosophy and art.},
  booktitle = {Wikipedia},
  urldate = {2019-10-23},
  date = {2019-09-22T05:22:57Z},
  note = {Page Version ID: 917083876}
}

@inreference{NashEquilibrium2019,
  langid = {english},
  title = {Nash Equilibrium},
  url = {https://en.wikipedia.org/w/index.php?title=Nash_equilibrium&oldid=920159762},
  abstract = {In game theory, the Nash equilibrium, named after the mathematician John Forbes Nash Jr., is a proposed solution of a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy.In terms of game theory, if each player has chosen a strategy, and no player can benefit by changing strategies while the other players keep theirs unchanged, then the current set of strategy choices and their corresponding payoffs constitutes a Nash equilibrium.
Stated simply, Alice and Bob are in Nash equilibrium if Alice is making the best decision she can, taking into account Bob's decision while his decision remains unchanged, and Bob is making the best decision he can, taking into account Alice's decision while her decision remains unchanged. Likewise, a group of players are in Nash equilibrium if each one is making the best decision possible, taking into account the decisions of the others in the game as long as the other parties' decisions remain unchanged.
Nash showed that there is a Nash equilibrium for every finite game: see further the article on strategy.},
  booktitle = {Wikipedia},
  urldate = {2019-10-23},
  date = {2019-10-08T00:45:45Z},
  file = {/home/jan/Zotero/storage/5F7DTR74/index.html},
  note = {Page Version ID: 920159762}
}

@inreference{VideoGameIndustry2019,
  langid = {english},
  title = {Video Game Industry},
  url = {https://en.wikipedia.org/w/index.php?title=Video_game_industry&oldid=920950695},
  abstract = {The video game industry is the economic sector involved in the development, marketing, and monetization of video games. It encompasses dozens of job disciplines and its component parts employ thousands of people worldwide.},
  booktitle = {Wikipedia},
  urldate = {2019-10-23},
  date = {2019-10-12T22:57:58Z},
  file = {/home/jan/Zotero/storage/HICGDHWY/index.html},
  note = {Page Version ID: 920950695}
}

@inreference{SuperMarioWorld2019,
  langid = {english},
  title = {\emph{Super }{{\emph{Mario World}}}},
  url = {https://en.wikipedia.org/w/index.php?title=Super_Mario_World&oldid=922683724},
  abstract = {Super Mario World is a 1990 side-scrolling platform game developed and published by Nintendo for the Super Nintendo Entertainment System (SNES). The story follows Mario's quest to save Princess Toadstool and Dinosaur Land from the series antagonist Bowser and his minions, the Koopalings. The gameplay is similar to that of earlier Super Mario games: Players control Mario or his brother Luigi through a series of levels in which the goal is to reach the flagpole at the end. Super Mario World introduced Yoshi, a dinosaur who can eat enemies and gain abilities by eating the shells of Koopa Troopas.
Nintendo Entertainment Analysis \& Development developed the game, led by director Takashi Tezuka and producer and series creator Shigeru Miyamoto. It is the first Mario game for the SNES and was designed to make the most of the console's technical features. The development team had more freedom compared to the series instalments for the Nintendo Entertainment System (NES). Yoshi was conceptualised during the development of the NES games but was not used until Super Mario World due to hardware limitations.
Super Mario World is often considered one of the greatest video games of all time. It sold over 20 million copies worldwide, making it the best-selling SNES game. It also led to an animated television series of the same name and a prequel, Yoshi's Island, released in August and October 1995. It has been rereleased on multiple occasions: It was part of the 1994 compilation Super Mario All-Stars + Super Mario World for the SNES and was rereleased for the Game Boy Advance as Super Mario World: Super Mario Advance 2 in 2001, on the Virtual Console for the Wii, Wii U, and New Nintendo 3DS consoles, and as part of the Super NES Classic Edition. It was also released on the Nintendo Switch through Nintendo Switch Online using the Super Nintendo Entertainment System app. Super Mario World is also one of the game styles available in Super Mario Maker and Super Mario Maker 2.},
  booktitle = {Wikipedia},
  urldate = {2019-10-23},
  date = {2019-10-23T17:11:49Z},
  file = {/home/jan/Zotero/storage/37L4AXTI/index.html},
  note = {Page Version ID: 922683724}
}

@online{FuSoYaNicheLunar,
  title = {{{FuSoYa}}'s {{Niche}} - {{Lunar Magic SMW Editor Introduction}}},
  url = {https://fusoya.eludevisibility.org/lm/index.html},
  urldate = {2019-10-28},
  file = {/home/jan/Zotero/storage/57YTP374/index.html}
}

@online{SMWCentralYour,
  title = {{{SMW Central}} - {{Your}} Primary {{Super Mario World}} Hacking Resource},
  url = {https://www.smwcentral.net/},
  urldate = {2019-10-28},
  file = {/home/jan/Zotero/storage/AS8HUELK/www.smwcentral.net.html}
}

@inreference{SuperMetroid2019,
  langid = {english},
  title = {\emph{Super }{{\emph{Metroid}}}},
  url = {https://en.wikipedia.org/w/index.php?title=Super_Metroid&oldid=919682616},
  abstract = {Super Metroid is an action-adventure game developed and published by Nintendo for the Super Nintendo Entertainment System in 1994. It is the third installment in the Metroid series, following the events of the Game Boy game Metroid II: Return of Samus (1991). Players control bounty hunter Samus Aran, who travels to planet Zebes to retrieve an infant Metroid creature stolen by the Space Pirate leader Ridley.
The gameplay focuses on exploration, with the player searching for power-ups that are used to reach previously inaccessible areas. It features new concepts to the series, such as the inventory screen, an automap, and the ability to fire in all directions. The development staff from previous Metroid games‚Äîincluding Yoshio Sakamoto, Makoto Kano and Gunpei Yokoi‚Äîreturned to develop Super Metroid over the course of two years, with half a year earlier to gain approval for the initial idea. The developers wanted to make a true action game, and to set the stage for Samus's reappearance.
The game received critical acclaim, praising its atmosphere, gameplay, music and graphics. It is often cited as one of the best video games of all time. Although the game did not sell well in Japan, it fared better in North America and had shipped 1.42 million copies worldwide by late 2003. Super Metroid, alongside Castlevania: Symphony of the Night (1997), is credited for establishing the "Metroidvania" subgenre, and has inspired other games within the genre. It also became popular among players for speedrunning. The game was followed by the 2002 release of Metroid Fusion and Metroid Prime, ending the series' eight-year hiatus. It was emulated for the Virtual Console service on various Nintendo platforms since 2007 and as part of the Super NES Classic Edition microconsole in 2017.},
  booktitle = {Wikipedia},
  urldate = {2019-11-04},
  date = {2019-10-05T03:24:35Z},
  file = {/home/jan/Zotero/storage/R9L4KH58/index.html},
  note = {Page Version ID: 919682616}
}

@software{andrejKarpathyCharrnn2019,
  title = {Karpathy/Char-Rnn},
  url = {https://github.com/karpathy/char-rnn},
  abstract = {Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch},
  urldate = {2019-11-05},
  date = {2019-11-05T02:36:09Z},
  author = {Andrej},
  origdate = {2015-05-21T17:25:24Z}
}

@software{OpenaiGpt22019,
  title = {Openai/Gpt-2},
  url = {https://github.com/openai/gpt-2},
  abstract = {Code for the paper "Language Models are Unsupervised Multitask Learners"},
  organization = {{OpenAI}},
  urldate = {2019-11-05},
  date = {2019-11-05T10:44:10Z},
  origdate = {2019-02-11T04:21:59Z}
}

@article{radfordLanguageModelsAre,
  langid = {english},
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  pages = {24},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  file = {/home/jan/Zotero/storage/5CLSZ7RY/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long {{Short}}-Term {{Memory}}},
  volume = {9},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  journaltitle = {Neural computation},
  shortjournal = {Neural computation},
  date = {1997-12-01},
  pages = {1735-80},
  author = {Hochreiter, Sepp and Schmidhuber, J√ºrgen},
  file = {/home/jan/Zotero/storage/2E34TL65/Hochreiter and Schmidhuber - 1997 - Long Short-term Memory.pdf}
}

@article{radfordUnsupervisedRepresentationLearning2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06434},
  primaryClass = {cs},
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1511.06434},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  urldate = {2019-11-06},
  date = {2016-01-07},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  file = {/home/jan/Zotero/storage/B8YX5DCW/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf;/home/jan/Zotero/storage/JNAXAKRI/1511.html}
}

@article{arjovskyWassersteinGAN2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.07875},
  primaryClass = {cs, stat},
  title = {Wasserstein {{GAN}}},
  url = {http://arxiv.org/abs/1701.07875},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  urldate = {2019-11-06},
  date = {2017-12-06},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L√©on},
  file = {/home/jan/Zotero/storage/KX5RM663/Arjovsky et al. - 2017 - Wasserstein GAN.pdf;/home/jan/Zotero/storage/V72BAI6B/1701.html}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  volume = {15},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  shorttitle = {Dropout},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2019-11-07},
  date = {2014},
  pages = {1929-1958},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  file = {/home/jan/Zotero/storage/VE5Q4GXZ/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf;/home/jan/Zotero/storage/NUQBKGH6/srivastava14a.html}
}

@article{ioffeBatchNormalizationAccelerating2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03167},
  primaryClass = {cs},
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  url = {http://arxiv.org/abs/1502.03167},
  shorttitle = {Batch {{Normalization}}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  urldate = {2019-11-07},
  date = {2015-03-02},
  keywords = {Computer Science - Machine Learning},
  author = {Ioffe, Sergey and Szegedy, Christian},
  file = {/home/jan/Zotero/storage/NNM7KCEP/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;/home/jan/Zotero/storage/68JHJXZC/1502.html}
}

@article{kingmaAdamMethodStochastic2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  url = {http://arxiv.org/abs/1412.6980},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate = {2019-11-07},
  date = {2017-01-29},
  keywords = {Computer Science - Machine Learning},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/home/jan/Zotero/storage/YJ57NCSD/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/jan/Zotero/storage/XABZYVAJ/1412.html}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  title = {Generative {{Adversarial Networks}}},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate = {2019-11-14},
  date = {2014-06-10},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  file = {/home/jan/Zotero/storage/Q8QQX7FG/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/home/jan/Zotero/storage/WWR2MPN6/1406.html}
}

@software{PytorchExamples2019,
  title = {Pytorch/Examples},
  url = {https://github.com/pytorch/examples},
  abstract = {A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.},
  organization = {{pytorch}},
  urldate = {2019-11-14},
  date = {2019-11-14T11:51:31Z},
  origdate = {2016-08-24T03:12:48Z}
}

@software{martinarjovskyMartinarjovskyWassersteinGAN2019,
  title = {Martinarjovsky/{{WassersteinGAN}}},
  url = {https://github.com/martinarjovsky/WassersteinGAN},
  abstract = {Contribute to martinarjovsky/WassersteinGAN development by creating an account on GitHub.},
  urldate = {2019-11-14},
  date = {2019-11-14T08:38:05Z},
  author = {{martinarjovsky}},
  origdate = {2017-01-30T06:51:23Z}
}

@software{FluxMLFluxJl2019,
  title = {{{FluxML}}/{{Flux}}.Jl},
  url = {https://github.com/FluxML/Flux.jl},
  abstract = {Relax! Flux is the ML library that doesn't make you tensor},
  organization = {{Flux}},
  urldate = {2019-11-14},
  date = {2019-11-14T16:01:29Z},
  keywords = {data-science,deep-learning,flux,machine-learning,neural-networks,the-human-brian},
  origdate = {2016-04-01T21:11:05Z}
}

@software{JuliaIOJLD2Jl2019,
  title = {{{JuliaIO}}/{{JLD2}}.Jl},
  url = {https://github.com/JuliaIO/JLD2.jl},
  abstract = {HDF5-compatible file format in pure Julia. Contribute to JuliaIO/JLD2.jl development by creating an account on GitHub.},
  organization = {{JuliaIO}},
  urldate = {2019-11-14},
  date = {2019-11-10T09:06:17Z},
  origdate = {2015-07-02T21:59:50Z}
}

@software{JuliaIOBSONJl2019,
  title = {{{JuliaIO}}/{{BSON}}.Jl},
  url = {https://github.com/JuliaIO/BSON.jl},
  abstract = {Contribute to JuliaIO/BSON.jl development by creating an account on GitHub.},
  organization = {{JuliaIO}},
  urldate = {2019-11-14},
  date = {2019-11-13T23:57:01Z},
  origdate = {2018-02-23T01:40:48Z}
}

@software{JuliaGPUCuArraysJl2019,
  title = {{{JuliaGPU}}/{{CuArrays}}.Jl},
  url = {https://github.com/JuliaGPU/CuArrays.jl},
  abstract = {A Curious Cumulation of CUDA Cuisine. Contribute to JuliaGPU/CuArrays.jl development by creating an account on GitHub.},
  organization = {{JuliaGPU}},
  urldate = {2019-11-14},
  date = {2019-11-14T14:24:46Z},
  keywords = {cuda,gpu-programming,julia},
  origdate = {2017-07-25T14:47:08Z}
}

@software{JuliaIOJLDJl2019,
  title = {{{JuliaIO}}/{{JLD}}.Jl},
  url = {https://github.com/JuliaIO/JLD.jl},
  abstract = {Saving and loading julia variables while preserving native types},
  organization = {{JuliaIO}},
  urldate = {2019-11-14},
  date = {2019-11-05T14:11:51Z},
  keywords = {data-storage,hdf5-format,jld-format,julia,serializer},
  origdate = {2015-07-15T12:31:16Z}
}

@inreference{MultilayerPerceptron2019,
  langid = {english},
  title = {Multilayer Perceptron},
  url = {https://en.wikipedia.org/w/index.php?title=Multilayer_perceptron&oldid=923044913},
  abstract = {A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). The term MLP is used ambiguously, sometimes loosely to refer to any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see ¬ß Terminology. Multilayer perceptrons are sometimes colloquially referred to as "vanilla" neural networks, especially when they have a single hidden layer.An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.},
  booktitle = {Wikipedia},
  urldate = {2019-11-14},
  date = {2019-10-26T00:10:04Z},
  file = {/home/jan/Zotero/storage/GU8LY88Y/index.html},
  note = {Page Version ID: 923044913}
}

@inreference{RectifierNeuralNetworks2019,
  langid = {english},
  title = {Rectifier (Neural Networks)},
  url = {https://en.wikipedia.org/w/index.php?title=Rectifier_(neural_networks)&oldid=923288576},
  abstract = {In the context of artificial neural networks, the rectifier is an activation function defined as the positive part of its argument:

  
    
      
        f
        (
        x
        )
        =
        
          x
          
            +
          
        
        =
        max
        (
        0
        ,
        x
        )
        ,
      
    
    \{\textbackslash{}displaystyle f(x)=x\^\{+\}=\textbackslash{}max(0,x),\}
  where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. 
This activation function was first introduced to a dynamical network by Hahnloser et al. in 2000 with strong biological motivations and mathematical justifications. It has been demonstrated for the first time in 2011 to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, as of 2017, the most popular activation function for deep neural networks.A unit employing the rectifier is also called a rectified linear unit (ReLU).Rectified linear units find applications in computer vision and speech recognition using deep neural nets.},
  booktitle = {Wikipedia},
  urldate = {2019-11-14},
  date = {2019-10-27T16:05:20Z},
  file = {/home/jan/Zotero/storage/2J39PCWY/index.html},
  note = {Page Version ID: 923288576}
}

@online{CuSPARSE,
  langid = {american},
  title = {{{cuSPARSE}}},
  url = {http://docs.nvidia.com/cuda/cusparse/index.html},
  abstract = {The API reference guide for cuSPARSE, the CUDA sparse matrix library.},
  type = {concept},
  urldate = {2019-11-14},
  file = {/home/jan/Zotero/storage/RCAU2MCB/index.html}
}

@online{SavingArrayLength,
  langid = {english},
  title = {Saving Array Length for Undef Array Issues by {{Codyk12}} ¬∑ {{Pull Request}} \#47 ¬∑ {{JuliaIO}}/{{BSON}}.Jl},
  url = {https://github.com/JuliaIO/BSON.jl/pull/47},
  abstract = {Changed saving format to save array length as item in the sparse dict when writing arrays. Uses the length key while reading the array to allocate the necessary length of the array. Backwards compa...},
  journaltitle = {GitHub},
  urldate = {2019-11-14},
  file = {/home/jan/Zotero/storage/KYPXCPPM/47.html}
}

@online{MaybeFixTypename,
  langid = {english},
  title = {Maybe Fix Typename for Nested Modules by Richiejp ¬∑ {{Pull Request}} \#126 ¬∑ {{JuliaIO}}/{{JLD2}}.Jl},
  url = {https://github.com/JuliaIO/JLD2.jl/pull/126},
  abstract = {I found that in my Julia 1.0+ project: type names in a nested module (or even just Core.String) had duplicate module names in the path. e.g. typname(A.B.C.T) = \&quot;A.B.C.C.T\&quot; and also typena...},
  journaltitle = {GitHub},
  urldate = {2019-11-14},
  file = {/home/jan/Zotero/storage/FCUN7UXY/126.html}
}

@software{peterChengchingwenTransformersJl2019,
  title = {Chengchingwen/{{Transformers}}.Jl},
  url = {https://github.com/chengchingwen/Transformers.jl},
  abstract = {Julia Implementation of Transformer models. Contribute to chengchingwen/Transformers.jl development by creating an account on GitHub.},
  urldate = {2019-11-14},
  date = {2019-11-14T18:57:01Z},
  keywords = {attention,deep-learning,flux,machine-learning,natural-language-processing,nlp,transformer},
  author = {Peter},
  origdate = {2018-12-27T06:24:53Z}
}

@software{JuliaComputingJuliaDBJl2019,
  title = {{{JuliaComputing}}/{{JuliaDB}}.Jl},
  url = {https://github.com/JuliaComputing/JuliaDB.jl},
  abstract = {Parallel analytical database in pure Julia. Contribute to JuliaComputing/JuliaDB.jl development by creating an account on GitHub.},
  organization = {{Julia Computing, Inc.}},
  urldate = {2019-11-14},
  date = {2019-11-12T16:43:53Z},
  origdate = {2017-02-01T17:35:53Z}
}

@software{JuliaGPUCUDAnativeJl2019,
  title = {{{JuliaGPU}}/{{CUDAnative}}.Jl},
  url = {https://github.com/JuliaGPU/CUDAnative.jl},
  abstract = {Julia support for native CUDA programming. Contribute to JuliaGPU/CUDAnative.jl development by creating an account on GitHub.},
  organization = {{JuliaGPU}},
  urldate = {2019-11-14},
  date = {2019-11-14T08:24:49Z},
  keywords = {cuda,cuda-toolkit,julia,julia-library},
  origdate = {2016-05-13T13:08:13Z}
}

@software{JuliaDataCSVJl2019,
  title = {{{JuliaData}}/{{CSV}}.Jl},
  url = {https://github.com/JuliaData/CSV.jl},
  abstract = {Utility library for working with CSV and other delimited files in the Julia programming language},
  organization = {{Julia Data}},
  urldate = {2019-11-14},
  date = {2019-11-13T00:48:33Z},
  origdate = {2015-05-28T19:08:56Z}
}

@software{vicentiniPhilipVincTensorBoardLoggerJl2019,
  title = {{{PhilipVinc}}/{{TensorBoardLogger}}.Jl},
  url = {https://github.com/PhilipVinc/TensorBoardLogger.jl},
  abstract = {Log to TensorBoard from Julia. Contribute to PhilipVinc/TensorBoardLogger.jl development by creating an account on GitHub.},
  urldate = {2019-11-14},
  date = {2019-10-24T12:57:51Z},
  keywords = {julia,logging,tensorboard},
  author = {Vicentini, Filippo},
  origdate = {2019-01-27T21:15:07Z}
}

@software{TensorflowTensorboard2019,
  title = {Tensorflow/Tensorboard},
  url = {https://github.com/tensorflow/tensorboard},
  abstract = {TensorFlow's Visualization Toolkit. Contribute to tensorflow/tensorboard development by creating an account on GitHub.},
  organization = {{tensorflow}},
  urldate = {2019-11-14},
  date = {2019-11-14T17:59:03Z},
  origdate = {2017-05-15T20:08:07Z}
}

@online{TensorBoard,
  langid = {english},
  title = {{{TensorBoard}}},
  url = {https://www.tensorflow.org/tensorboard},
  journaltitle = {TensorFlow},
  urldate = {2019-11-14},
  file = {/home/jan/Zotero/storage/5GGU93BX/tensorboard.html}
}

@software{FluxMLZygoteJl2019,
  title = {{{FluxML}}/{{Zygote}}.Jl},
  url = {https://github.com/FluxML/Zygote.jl},
  abstract = {Intimate Affection Auditor. Contribute to FluxML/Zygote.jl development by creating an account on GitHub.},
  organization = {{Flux}},
  urldate = {2019-11-14},
  date = {2019-11-14T14:21:30Z},
  keywords = {automatic-differentiation,control-flow,gradient,julia,julia-compiler,machine-learning},
  origdate = {2018-08-06T22:08:26Z}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  volume = {59},
  issn = {0036-1445},
  url = {https://epubs.siam.org/doi/10.1137/141000671},
  doi = {10.1137/141000671},
  shorttitle = {Julia},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be ‚Äúlaws of nature"  by practitioners of numerical computing: \textbackslash{}beginlist \textbackslash{}item  High-level dynamic programs have to be slow. \textbackslash{}item  One must prototype in one language and then rewrite in another language for speed or deployment. \textbackslash{}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \textbackslash{}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
  number = {1},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  urldate = {2019-11-14},
  date = {2017-01-01},
  pages = {65-98},
  author = {Bezanson, Jeff. and Edelman, Alan. and Karpinski, Stefan. and Shah, Viral B.},
  file = {/home/jan/Zotero/storage/XIE85V37/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf;/home/jan/Zotero/storage/UKLXNV5U/141000671.html}
}

@article{makhzaniAdversarialAutoencoders2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.05644},
  primaryClass = {cs},
  title = {Adversarial {{Autoencoders}}},
  url = {http://arxiv.org/abs/1511.05644},
  abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  urldate = {2019-11-18},
  date = {2016-05-24},
  keywords = {Computer Science - Machine Learning},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  file = {/home/jan/Zotero/storage/V6DUNNTT/Makhzani et al. - 2016 - Adversarial Autoencoders.pdf;/home/jan/Zotero/storage/4KD4SAT3/1511.html}
}


