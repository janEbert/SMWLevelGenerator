\subsection{Training Pipeline}
\label{sec:training-pipeline}

Our training pipeline for each type of model consists of two major
parts: the data iterator which reads and preprocesses our data and the
actual training loop acting on the data we obtain from the iterator. \\
Let us first focus on the data iterators and after that on the
training loops.

\subsubsection{Data Iterators}

Since the data iterator is responsible for shaping our data to the
form expected by the model, we still need to apply some more
preprocessing to the already preprocessed data contained in the
database. As this may be a large bottleneck, we focused on optimizing
data iteration (on any Julia version 1.3 or later, this bottleneck can
mostly be avoided by simply using enough data iterator threads). As we
explained in section~\ref{sec:preprocessing}, our data is stored
differently if using the highest compression level. We also explained
that storing our 3-dimensional data in ``columnar'' layers (sliced
along the second dimension) is more performant. With the columnar
format, we can simply concatenate the individual matrices and obtain
our level in a cache-efficient way. \\
All data iterators use thread-safe channels which the user can take
values out of without having to worry about race conditions and
synchronicity.

For optimization purposes related to actual training on the data, we
minimize the amount of data per batch for models that require padded
data. For example, batches for transformer models only contain columns
up to the maximum level length per batch instead of up to the maximum
level length over all levels (512~columns for horizontal levels).

We do not normalize our data in any way. Normalizing the mean would
mean losing the sparsity patterns which improve computational
efficiency by a large margin. We decided against normalizing the
covariance due to the differences in the way we shape our data for
each model and the subsequent influences on unrelated results. First
off, we cannot calculate the covariance of the unmodified dataset due
to the differences in sequence lengths. While we could easily compute
the covariance of our dataset by padding all sequences to the same
length, using this result to normalize data for \emph{all} models
would affect~-- for example~-- sequence models that allow sequences of
different lengths in an undesired way. This is because we normalize
their input data based on data that is nonexistent in their view (the
padded zeros).

With the added randomness of the order the data iterator threads
output their data (due to operating system scheduling and other
factors~-- this randomness is not affected by a random number generator
seed), we cannot guarantee reproducible results if using threads.
While we could adjust the iterators to synchronize so results are
perfectly reproducible, we decided to not implement this feature as we
assume this would largely hinder efficiency. In the future, an option
to enable perfect reproducibility would be a good improvement. \\
To minimize overhead, we keep each thread running in an endless loop,
meaning the user needs to keep track of how many training examples
they need for each epoch. In an earlier implementation, an iterable
data iterator was used but we decided this minor convenience was not
worth having to start multiple threads on each training and test
epoch.

The data iterator for first screen data works similar to the other but
has fewer options as we do not need to iterate over the first screen
but supply it as a whole. With this, we do not need to handle possibly
iterating over each tile and~-- with that option activated~-- in which
direction to iterate (each ``columnar'' slice from top to bottom or
bottom to top). We also have not found the need to worry about padding
and supporting different inputs for different models (as of yet).
Obviously, we still need to support the different dimensionalities of
data. \\
This data iterator also supplies a tuple of values for simplicity: the
first screen and the constant input of each level. This enables us to
reuse the iterator for both GAN and metadata predictor training.

\subsubsection{Training Loops}
\label{sec:training-loops}

Our training loops for all model types follow the same schematic:
\begin{enumerate}
\item Initialize parameters, data iterators, loggers, optionally load
  model, \dots
\item Store parameters
\item Initial test
\item Loop over all epochs:
  \begin{enumerate}
    \item Train model(s)
    \item Periodically test model and check for early stopping
    \item Periodically save model checkpoint
  \end{enumerate}
\item Save model checkpoint
\item Clean up resources
\end{enumerate}

All training loops accept either a model or the path to a model
checkpoint, correctly initializing everything from the checkpoint and
starting training from the exact point it left off (although this may
not always be the exact point due to the randomness in threaded data
iterators; we skip as many steps as are saved in the checkpoint). The
parameters that are the same over all training loops contain values
like the number of epochs, early stopping-related values, which
checkpointing method to use (as discussed in section~\ref{sec:julia},
we support saving as BSON or JLD2), after how many training steps to
log and save the model, the amount of data iterator threads, the
random seed and others. \\
The parameters are collectively combined with parameters stored in the
model (an optional specification of our abstract model interface) and
saved in a JSON file to combine both human readability and parsing.

To make debugging model prototypes easier, we implement the
\texttt{overfit\_on\_batch} parameter for all training loops, allowing
the user to overfit the model on a small batch of a default or chosen
size, thereby catching most early errors.

We already mentioned in section~\ref{sec:generation-via-prediction}
that our sequence prediction models implement a ``soft'' loss
function; these are also selected as a training parameter. \\
Due to the iterator allowing it, the GAN training loop allows training
both a GAN and a metadata predictor at the same time.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../SMWLevelGenerator"
%%% End:

