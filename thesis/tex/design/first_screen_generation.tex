\subsubsection{First Screen Generation}
\label{sec:design-gans}

Our 1-dimensional GANs automatically adjust their layers to the input
size. The GANs for training data in 2D and above are implemented with
manually chosen stride, padding and dilation so the output is the same
size as the first screen of the level. The input to the discriminators
of our GANs is the first screen of the level (a vector of 16 elements
in the 1-dimensional case, a $27 \times 16$ matrix in the
2-dimensional case and a $27 \times 16 \times l$ cube in the
3-dimensional case). Unlike our sequence models, neither the
discriminators nor the generators receive any constant inputs of
metadata (as the metadata is generated from the GANs' outputs).

We use the same abstract model class for our GANs that was used for
the time series prediction models, but extend it with two new classes
for discriminator and generator models. With this setup, we hope to
make extending the GAN pipeline with new models as simple as the
sequence prediction pipeline. Unlike the time series predictors,
discriminator models do not allow inputs other than those of the form
$r \times c \times l \times b$ or $c \times l \times b$, where $r$ is
the amount of rows, $c$ the amount of columns, $l$ the amount of
layers (channels in image processing) and $b$ the batch size.
Generator models only take a vector of latent noise of a size
determined during model creation. This vector is only given as a
matrix due to batching (the model handles this automatically).

The first convolutional layer of the discriminator is not followed by
a normalization layer. We follow the example of~
% TODO cite dcgan tut and wsgan
.

Our GANs have the following parameters for their convenience
constructors (these are once again the same for the DCGAN and
Wasserstein DCGAN, but with different defaults). It should also be
noted that while kernel size is adjustable, this requires building new
models as the expected sizes will be different. For the discriminators:
\begin{itemize}
\item amount of features in each convolutional layer's kernel; the
  features per successive layer are then multiplied by successive
  powers of two (starting from $2^{0}$ up to $2^{k - 2}$, where $k$ is
  the amount of convolutional layers\footnote{We get $k - 2$ because
    the start counting powers from zero and because the final
    convolutional layer shrinks the number of features to one.}).
\item normalization layer after each convolutional layer
\item activation function after each normalization layer
\item final activation function
\item kernel window size
\end{itemize}
For the generators:
\begin{itemize}
\item amount of features in each convolutional layer's kernel; the
  features sizes follow the same rules as the discriminator but in
  reverse (starting form $2^{k - 2}$ and shrinking towards $2^{0}$,
  where $k$ is the amount of convolutional layers).
\item normalization layer after each convolutional layer
\item activation function after each normalization layer
\item final activation function
\item kernel window size
\end{itemize}
The input and output sizes are also adjustable for each model.

After the GANs, the metadata predictor follows in our pipeline. That
model will be described in the next section.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../SMWLevelGenerator"
%%% End:

