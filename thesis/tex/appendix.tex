\section{Appendix}

Here, we will list some more details that were not out of the scope of
the thesis but more intended for a reader interested in further
programming and design aspects. We will first go more in detail on the
Julia packages used throughout the thesis and then describe the model
interface.

\subsection{Packages}
\label{sec:packages}

Let us now list the packages (excluding Julia's large standard
library) most important for our preprocessing and training pipeline.
While this list deliberately does not cover all packages used, we hope
to give an overview of what the ecosystem made or will make possible,
why these packages were important for us and what issues we still had
to solve manually.

We encountered several issues with model checkpoints. We first
switched from an unmerged pull request branch of \\
\mbox{BSON.jl}~\cite{JuliaIOBSONJl2019,keslerSavingArrayLength} (which
contains an issue only later addressed in the master branch) to the
HDF5-based \mbox{JLD.jl}~\cite{JuliaIOJLDJl2019}, then to the more
unstable \mbox{JLD2.jl}~\cite{JuliaIOJLD2Jl2019} due to not being able
to save functions in \mbox{JLD.jl}. With \mbox{JLD2.jl}, there were
still errors which led us to also check out a pull request for this
package~\cite{palethorpeMaybeFixTypename} (this has been merged but
for stability reasons, we are not ready to switch to the master branch
of \mbox{JLD2.jl} just yet). Finally, we settled with \mbox{JLD2.jl}
for our model checkpoints as the most stable package. While the files
are not as convenient as those of \mbox{BSON.jl}, we are sure the
stability is worth it. In the end, we support saving to either
\mbox{BSON.jl} or \mbox{JLD2.jl} with \mbox{JLD2.jl} as the default.

For the databases containing our training data, we used
\mbox{JuliaDB.jl}~\cite{JuliaComputingJuliaDBJl2019} as it supports
parallel and out-of-core processing of the database. While these
features have not been used yet, they should future-proof our program
for cluster computing and databases that do not fit into memory on a
host PC. Smaller statistics (especially pre-computed ones that we load
to reduce startup time) were saved as CSV files which
\mbox{CSV.jl}~\cite{JuliaDataCSVJl2019} was a great fit for.

For our machine learning needs, we used
\mbox{Flux.jl}~\cite{FluxMLFluxJl2019}. While \mbox{Flux.jl} is one of
the most mature deep learning frameworks in Julia, it still missed
some basic functionality. For example, as mentioned in
section~\ref{sec:first-screen-generation}, we had to define our own
convolutional layer without bias as it had not been included at the
time of writing. With \mbox{Zygote.jl}~\cite{FluxMLZygoteJl2019}, a
framework supporting source-to-source automatic differentiation that
is to be included in \mbox{Flux.jl}, we hope to gain a large speed
boost in the future with compiled derivatives as they will not have to
be tracked at runtime. \\
Due to \mbox{Flux.jl}'s GPU support enabled by
\mbox{CuArrays.jl}~\cite{JuliaGPUCuArraysJl2019}, we were able to
achieve great performance even while writing specialized loss
functions. We were sadly not able to support the CUDA library for
sparse arrays, \mbox{cuSPARSE}; due to the \mbox{CuArrays.jl} package
not supporting all the features necessary for us (for example simple
primitives like array slicing) for \mbox{cuSPARSE} arrays, we had to
comment out the few lines pertaining to sparse CUDA arrays at the
moment. These issues were not in the scope of this thesis but will be
addressed at a later point, surely granting another large speedup to
any GPU-run training pipeline that uses sparse matrices (the training
pipeline is explained in section~\ref{sec:training-pipeline}).
Another, more low-level GPU package,
\mbox{CUDAnative.jl}~\cite{JuliaGPUCUDAnativeJl2019}, allowed us to
write our own CUDA kernels with ease. This enabled faster clamping of
the parameters of our Wasserstein GANs. \\
The \mbox{Transformers.jl}~\cite{peterChengchingwenTransformersJl2019}
package was essential in getting the \mbox{GPT-2} transformer model to
run. While the model is not included in the package as of the writing
of this thesis, the given building blocks were a large help. \\
Finally,
\mbox{TensorBoardLogger.jl}~\cite{vicentiniPhilipVincTensorBoardLoggerJl2019}
enabled a great tool for monitoring training progress:
TensorBoard~\cite{TensorBoard}.

\subsection{Model Interface}
\label{sec:model-interface}

While the source code already gives a good documentation of the model
interface, for completeness sake, we will give a brief overview here
as well. Our model interface \texttt{LearningModel} is an abstract
class that implements several methods that may be overwritten. For a
class \texttt{YourModel} to be a \texttt{LearningModel}, the model
needs to (1)~be defined as a \texttt{Flux.@treelike} (in future
versions as a \texttt{Flux.@functor}; this is simply a line consisting
of \texttt{Flux.@treelike YourModel}) to enable parameter tracking and
several convenience methods. It needs to (2)~implement a field
\texttt{hyperparams} of type \texttt{Dict\{Symbol, Any\}} which may
hold any parameters that discern the model from a model of the same
type (these are optional). (3)~The \texttt{hyperparams} dictionary has
one required key, the \texttt{:dimensionality} of the model as a
\texttt{Symbol} so data may be correctly shaped for the model (this is
any of the types of training data we listed in
section~\ref{sec:preprocessing}).

(4)~A \texttt{LearningModel} model for sequence prediction has to
implement the following functions:
\begin{itemize}
\item \texttt{(model::YourModel)(input)}: the application function to
  enable calling the model on input data.
\item \texttt{makeloss(model::YourModel, criterion)}: return a \\
  2-argument loss function by applying the given criterion (for
  example, a criterion is \texttt{Flux.mse}).
\item \texttt{dataiteratorparams(model::YourModel)}: return
  parameters for the data iterator (does the model expect data as a
  vector of vectors, in matrix form, as a joined vector, \dots).
\item \texttt{calculate\_loss(model::YourModel, loss, input, target)}:
  return the loss obtained by applying the loss function to the given
  input with the given target (this is mostly the function returned by
  \texttt{makeloss}, but with optional preparation of the model).
\item \texttt{step!(model::YourModel, parameters, optimizer, loss,
    input, target)}: update the given model with a single training
  step on the given input with the given target and return the loss; a
  single training step.
\item (optionally) \texttt{makesoftloss(model::YourModel, criterion)}:
  like \texttt{makeloss} but return a ``soft'' loss function
  penalizing wrong predictions less (this can of course just be used
  as any other alternative loss function).
\end{itemize}

GANs and metadata predictors do not need to implement
\texttt{dataiteratorparams} and the optional \texttt{makesoftloss} as
they are not used. However, some functions expect different arguments.
For these, we refer a curious reader to the source code. \\
Finally, the generator models of GANs are required to save the input
size of their latent vectors under the key \texttt{:inputsize}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../SMWLevelGenerator"
%%% End:
