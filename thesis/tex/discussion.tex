\section{Discussion}

The experiments once again show how well neural networks generalize.
In our case, this generalization was a bit too much and we should look
for ways to further de-regularize.

Due to most of the work relying on the sequence prediction models,
these are the weakest link of our pipeline. What we found out is that
the models were not able to memorize the data but instead minimized
error by heavily generalizing. While we tried giving the models more
layers, this resulted in much higher and not decreasing loss.

\subsection{Related Work}



\subsection{Future Improvements}

While the experiments have shown that our approach to generating
levels via sequence prediction have not worked out yet, we have many
suggestions and possible solutions that ought to be tried out in the
future. We also have several improvements planned for the project
which are out of the scope of this thesis. \\
As is usual in deep learning, for all our models, tuning the
hyperparameters further may lead to surprising results. While we do
not believe~-- looking at the experiments and our prior discussion
above~-- that only changing hyperparameters will lead to a good level
generator, with deep learning you never know if the combined changes
over many parameters may suddenly lead to a good result. \\
We also believe the training pipeline in general would benefit from
further features such as learning rate regulation (especially for
warm-up).

In terms of the sequence prediction models, first off, we should try
training on generated data that goes back some amount steps into the
past instead of only on ``perfectly'' predicted data~-- the actual,
ground truth data (we mentioned this in
section~\ref{sec:generation-via-prediction}). This would allow
minimizing errors for future predictions. As we see, we cannot rely on
the model to predict the next column perfectly, meaning an error for
each predicted column will accumulate and result in worse and worse
predictions.

We could try implementing more loss functions akin to the ``soft''
loss we introduced in section~\ref{sec:generation-via-prediction} as
well. Another promising improvement would be implementing adversarial
sequence
generators~\cite{yuSeqGANSequenceGenerative2017,liAdversarialDiscreteSequence}
which we had initially foregone due to considerations regarding the
high dimensionality of our data. With those, the GANs and~-- possibly~--
the metadata predictor become redundant, leading us to a single model
capable of generating complete levels from scratch.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../SMWLevelGenerator"
%%% End:

