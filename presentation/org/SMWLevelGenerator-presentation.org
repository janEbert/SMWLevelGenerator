#+options: ':nil *:t -:t ::t <:t \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+options: timestamp:t title:t toc:t todo:t |:t
#+title: [[https://github.com/janEbert/SMWLevelGenerator][SMWLevelGenerator]]
#+author: [[mailto:janpublicebert@posteo.de][Jan Ebert]]
#+email: janpublicebert@posteo.de
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.3 (Org mode 9.3.1)
#+latex_header:
#+latex_header_extra:
#+description:
#+keywords:
#+subtitle: Generating Super Mario World Levels Using \newline Deep Neural Networks
#+date: January 13, 2020
#+options: H:3
#+startup: beamer
#+latex_class: beamer
#+latex_class_options: [presentation]
#+latex_header: \usetheme[sectionpage=progressbar,progressbar=frametitle]{metropolis}
#+latex_header: \setbeamercolor{alerted text}{fg=mLightGreen}
#+latex_header: \setbeamercolor{progress bar}{fg=mLightGreen}
#+latex_header: \usepackage{listings}
#+latex_header: \lstset{language=Octave,commentstyle=\color{gray},keywordstyle=\color{green!40!black},identifierstyle=\color{blue},tabsize=4}
#+columns: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+beamer_theme: default
#+beamer_color_theme:
#+beamer_font_theme:
#+beamer_inner_theme:
#+beamer_outer_theme:
#+beamer_header:

#+name: setup-listings
#+begin_src emacs-lisp :exports results :results silent
(setq org-latex-listings 'listings)
(setq org-latex-custom-lang-environments
      '((emacs-lisp "common-lispcode")))
(setq org-latex-listings-options
      '(("frame" "lines")
        ("basicstyle" "\\footnotesize\\ttfamily")
        ("numbers" "left")
        ("numberstyle" "\\tiny")))
(setq org-latex-to-pdf-process
      '("pdflatex -interaction nonstopmode -output-directory %o %f"
      "pdflatex -interaction nonstopmode -output-directory %o %f"
      "pdflatex -interaction nonstopmode -output-directory %o %f"))
(org-add-link-type
 "latex" nil
 (lambda (path desc format)
   (cond
    ((eq format 'html)
     (format "<span class=\"%s\">%s</span>" path desc))
    ((eq format 'latex)
     (format "\\%s{%s}" path desc)))))
#+end_src

* Managing Expectations
*** Goals
- Create an open and optimized research framework for level generation
  in an arbitrarily complex environment
- Challenge/increase existing complexity (papers usually on much
  simpler Super Mario Bros.)
- Analyze usage of sequence prediction for generation
- Compare sequence generation capabilities of LSTMs vs.@@beamer:\
  @@transformers
*** What /Not/ to Expect: The Chair
Do not expect a “working” level generator producing great results:
- Decreasing loss does not imply great levels
- Levels generated by my models are barebones and unplayable
- Evaluating results is tedious and has to be done manually due to no
  objective function
*** What to Expect: The Hammer
Expect an accessible, extensible, efficient, cross-platform framework
to implement, train and test models:
- Data (pre-)processing pipeline
- Highly compressed database
- Support for any desired data dimensionality
- Level generation using a combination of different methods
- Focus on simplicity for library user
*** With That Out of the Way...
- Source code, thesis and slides available at [[https://github.com/janEbert/SMWLevelGenerator]]
- Clickable links are invisible but should be obvious
* Fundamentals
** Super Mario World
*** Super Mario World (SMW)
- 1990 2D platformer by Nintendo for the Super Nintendo Entertainment
  System (SNES)
- Huge amount of tiles, enemies, interactions, ...
- Running, jumping, flying, tossing, riding dinosaur, ...
- Large amount of data due to active hacking scene
Let's keep it *@@beamer:<2->@@relatively* simple!
*** Hacking
- Active hacking community around SMW
- Hacks available as patches to original ROM due to copyright
- /Very/ heavy modifications possible; we keep it simple (“vanilla”
  hacks)
- [[https://smwcentral.net/][SMWCentral.net]] as main resource
- Hacks filtered by following criteria:
  - Rating \geq 3.0 (community average)
  - “vanilla” tag
- However, neither “vanilla”-ness nor quality of hacks is guaranteed
In the end: over 300 hacks with over @@beamer:17\,000@@ levels
\newline (over @@beamer:15\,000@@ after filtering).
*** Lunar Magic
[[https://fusoya.eludevisibility.org/lm/index.html][Lunar Magic]] (LM) is the main tool to modify SMW:
- GUI editing
- Advanced modifications via 65C816 assembly
- Many convenience modifications (e.g.@@beamer:\ @@extended level
  dimensions, placing secondary entrances anywhere, ...)
- Author /FuSoYa/ provided private build to support dumping and
  reading levels from and into ROMs (and answered many questions)
*** Short Lunar Magic Showcase                                      :B_frame:
	:PROPERTIES:
	:BEAMER_env: frame
	:BEAMER_opt: standout
	:END:
Live Demo
*** Super Mario World Levels In-Depth
- We will only focus on horizontal levels
- Levels are divided into *screens* (27 \times 16-rectangles)
- Levels contain metadata having influence on tileset, water, ...
- Levels are 3D tensors consisting of the following layers:
  - Tiles (512)
  - Main and midway entrance (2)
  - Sprites (enemies, special effects) (241)
  - Level exits (screen and secondary) (511 + 449)
  - Secondary entrances (449)
2164 layers in total! \pause With 27 rows and 512 columns at maximum:
\approx @@beamer:\only<3>{30\,000\,000}\only<2>{300\,000}@@ binary
entries per level.
*** Level Example
[[../../thesis/img/Level105_clean.png]]
*** Level Detailed
[[../../thesis/img/Level105_detailed.png]]
*** Level Layers
[[../../thesis/img/Level105_layers_grid_border_grid_grouped_rainbow.png]]
*** Level Layers                                                    :B_frame:
	:PROPERTIES:
	:BEAMER_env: frame
	:BEAMER_opt: standout
	:END:
[[../../thesis/img/Level105_layers_grid_border_grid_grouped_rainbow.png]]
*** Vanilla and Custom Tiles
- Lunar Magic allows new tiles with custom behavior to be implemented
- Vanilla game features 512 unique tiles which may reference each
  other
- Custom tiles may use different graphics but reference vanilla
  behavior \newline \rightarrow Not all custom tiles are non-vanilla!
  Most people do not program new tiles but want custom graphics
- Lunar Magic rejects cyclical references
- Resolve custom tiles to vanilla tiles by following references
** Julia
*** Julia
- Implemented in [[https://julialang.org/][Julia 1.3]] (and 1.2)
- Modern dynamically typed language; combination of Lisp, Python and
  Octave with C-level performance
- JIT-compiled via LLVM
- Simple GPU usage and extensibility
- User-friendly multi threading and distributed programming
- Great REPL and package manager
- Easy to use: type stability and care with caching \rightarrow speed
*** Julia and CUDAnative.jl
Wasserstein GANs need clamping of parameters for convergence
properties. \newline Method in Julia stdlib: [[https://docs.julialang.org/en/v1/base/math/#Base.Math.clamp!][~clamp!(array, low,
high)~]]

\pause However! Slow on GPUs due to scalar indexing. :( \newline
\pause Solution: write it yourself -- in high-level Julia thanks to
[[https://github.com/JuliaGPU/CUDAnative.jl][CUDAnative.jl]] \pause (and make a pull request later).
*** DIY GPU Kernel                                                  :B_frame:
	:PROPERTIES:
	:BEAMER_env: frame
	:END:
We simply define a new ~clamp!~ method on GPU arrays:
#+begin_src octave :exports code
function Base.clamp!(a::CuArray, low, high)
    function kernel(a, low, high)
        I = CuArrays.@cuindex a
        a[I...] = clamp(a[I...], low, high)
        return
    end

    blocks, threads = CuArrays.cudims(a)
    @cuda(blocks=blocks, threads=threads,
          kernel(a, low, high))
    return a
end
#+end_src
*** Julia Profits
- Thesis resulted in multiple PRs all over the Julia ecosystem
- Due to combination of readability and efficiency, it was both easy
  and satisfying for me to contribute
- Writing in Julia made adding new features and functionality a breeze
  (sparse GPU array support in a few lines)
There is still a lot of work ahead but it is getting there.
*** Julia Ecosystem -- What to Look Out For
Hot at the moment and interesting for us:
- Deep learning frameworks [[https://github.com/FluxML/Flux.jl][Flux.jl]] (used for thesis) and [[https://github.com/denizyuret/Knet.jl][Knet.jl]]
- Source-to-source automatic differentiation via [[https://github.com/FluxML/Zygote.jl][Zygote.jl]] (was too
  unstable for me; recently became default AD engine for Flux.jl)
- TPU compilation via [[https://github.com/JuliaTPU/XLA.jl][XLA.jl]] (would have to use [[https://github.com/malmaud/TensorFlow.jl][TensorFlow.jl]])
- Many others including classical ML frameworks, toolkits and algorithms
* Framework
** Introduction
*** SMWLevelGenerator
The framework can be roughly divided into these modules:
- Data preprocessing and database generation
- Data iterators
- Model interface
- Training loops
- Level generation pipeline
*** Setup (Quick Version)
#+attr_beamer: :overlay +-
1. Get dependencies ([[https://julialang.org/][Julia 1.3]], [[https://www.tensorflow.org/][TensorBoard]], [[https://drive.google.com/uc?export=download&id=1WSsvEhWEZiIMc7W0kVZW3Z4m7_Qc0Ess][Lunar Magic]][fn::Private
   build], [[https://dl.smwcentral.net/11474/floating.zip][Floating IPS]], [[https://www.winehq.org/][Wine]] if not on Windows, Super Mario World
   ROM[fn::American version; CRC32 checksum =a31bead4=])
2. Instantiate Julia project: \newline
   ~julia --project -e "using Pkg; Pkg.instantiate()"~
3. [[https://drive.google.com/uc?export=download&id=1Ujr7l5lpRCO-EROOqobZyi8C-Eu3U4inf][Download databases]] and decompress
@@beamer:\uncover<+->{You're done; train and generate to your heart's
content.}@@
*** Setup (Manual/BTS Version)
Most of these are done in a single line; still, this gives an overview
of what happens behind the scenes.
#+attr_beamer: :overlay +-
1. Get dependencies and instantiate project (see previous slide)
2. Download desired hacks
3. Unzip hacks, patch ROMs and dump levels via scripts
4. Remove test, duplicate and “dirty” levels via scripts
5. Generate level statistics via Julia REPL
6. Generate database(s)
*** Pipeline Overview
#+attr_latex: :height 20em
[[../../thesis/img/pipeline.png]]
*** How Does It Work?
- Combination of different methods:
  1. *Generative methods* to generate initial inputs (first screen)
  2. *Image processing* to predict metadata from initial input
  3. *Natural language processing* to sequentially generate the rest
     of the level from the initial inputs
- What we will focus on: read level column by column, predict next
  column (tile by tile also possible)
- Each column contains constant metadata and bit whether level has
  /not/ ended
- Levels end with column of zeros (during generation, only the one bit
  matters)
- Loss: summed MSE of each predicted column in relation to target
  column
*** Why Regression and Not Classification?
For these kinds of tasks: usually use one-hot-encoding to predict the
“class” of the next tile/column. \newline I wanted to predict column
by column for speed and more local correlation.

Number of classes when reading...
- ... column by column: @@beamer:$\approx 17\,600$ \uncover<2->{\alert{digit} number}@@
- ... tile by tile: 662
\pause No way I could train a model on that many classes even if
memory problems were solved.
*** Dimensionalities
Different complexities reflected on the highest level via
dimensionality:
- 1D: Level is seen as single row of one type of tile.
- 2D: Level is seen as matrix of one type of tile.
- 3D: Level is seen as cube with chosen tile layers.
The default type of tile for 1D and 2D is the ground tile of the
level.
*** Simplifications
Too many to list (check out the thesis!), here are important ones:
- Levels are observed independently (connections by exits/entrances
  are ignored)
- A lot of metadata is omitted (unrelated to level generation)
- Test levels or unfinished levels are left in the dataset
- Levels are assumed to always go from left to right
- Several types of levels are excluded (vertical, boss, with layer 2
  interaction[fn::Usually a background layer but may be made
  interactive with sprite commands.])
** Preprocessing Pipeline
*** Data Preprocessing
- Lunar Magic dumps five different files for each level corresponding
  to different parts of the level
- Remove encrypted hacks (yep...) and those throwing errors
- Filter duplicate and vanilla game test levels by checksums
- Remove levels not adhering to vanilla behavior
- Format levels according to user-specified configuration
  (dimensionality, which layers, output type, ...)
*** Database Compression
- Maximum storage required per level if storing bits compactly[fn::One
  bit per bit, 8 bits per byte]: \newline 30 \cdot 10^6 bits \div 8 =
  3.75 MB
- With @@beamer:17\,000@@ levels: 3.75 MB \cdot @@beamer:17\,000@@
  \approx 63.75 GB
- Usable but too much for me; database should fit into 8 GB of
  RAM[fn::Free space on my laptop with @@beamer:2\,000@@ browser tabs
  open]
#+attr_beamer: :overlay +-
1. Use sparse arrays! Additional speed benefits for free \newline
   (0.046 *%* of data are assigned in full levels)
2. Use sparse arrays with smallest integer sizes covering full range
   (~Int64~ \rightarrow ~UInt16~)
3. Most layers are empty: do not save these either
*** Database Compression Results
Maximum calculated required storage[fn::@@beamer:17\,000 levels \cdot\
30\,000\,000$@@ entries = @@beamer:510\,000\,000\,000@@ entries in
total]: 63.75 GB \newline After highest compression: 430 MB

Due to recurring values, ~tar~ and ~gzip~ compress this further
\newline to 28 MB. \newline 7-Zip compresses to only 16 MB! Now
/that's/ portable.
*** Data Iterator
Optimizations:
- Sparse arrays already optimize our data iterator for large dimensionalities
- Sparse GPU arrays currently uncommented due to missing functionality
  in external package
- Arbitrarily many threads or single coroutine for data iterator
Different models require different data layouts; there are several
implementations that cover most cases (as matrix or as list of
columns/tiles, both with optional padding). \newline \pause All of
this behind the scenes due to...
*** Model Interface
Abstract type requiring minimal implementation to work with the
framework; adding new models requires the following:
1. Defined as ~Flux.@treelike~ (~Flux.@functor~ in future versions)
2. Field ~hyperparams~ of type ~Dict{Symbol, Any}~
3. Required key in ~hyperparams~[fn::GAN generators require one more
   key ~:inputsize~]: ~:dimensionality~ of model (a ~Symbol~)[fn::1D,
   2D, 3D only tiles, 3D, ... (very easily extensible)]
4. 5 required functions for sequence predictors, one less for
   GANs/metadata predictors
Any model implementing this interface works with the framework.
** Models
*** Sequence Predictors
- LSTM (stack)
- Transformer (GPT-2) (required data layout is not optimal for us)
- Random predictor (optimal activation chance by default)
Models may implement “soft” loss that penalizes incorrect predictions
less if the prior two elements were the same.

Predictions not done on predicted data \rightarrow error accumulation
not observed/reduced during training.

Inputs are levels from beginning to (current) end, outputs are the
predicted next column for each input column. Remember each input also
has a metadata vector attached; outputs do not.
*** GANs
- DCGAN
- Wasserstein DCGAN
- Dense Wasserstein GAN
1D GANs automatically adjust layers to input; 2D and above have
manually chosen stride, padding and dilation so output size matches
first screen size.

Discriminators: Inputs are first screen tensors (vector in 1D, matrix
in 2D, cube in 3D), outputs are scalars whether input is real.
\newline Generators: Inputs are noise vectors, outputs are first
screen tensors.
*** Image Processing Models
- Convolutional
- Dense (MLP)

Inputs are first screen tensors, outputs are the constant metadata
vectors also supplied to the sequence predictor.
*** Training Loops
- Checkpointing and resuming training
- Handling experiments (storing all parameters, logging, ...)
- Logging via TensorBoard
- Early stopping
- Overfitting on batch for debugging
Many other settings allow the exact modifications you want.
* Level Generation
*** Pipeline
The models are trained, what next?
1. Feed input data (generated or from database) and subsequent
   generations into sequence predictor until the “level has not
   ended”-bit is /not/ set or until the maximum level length is
   reached
2. Post-process
3. Revert all pre-processing
4. Write back to Lunar Magic-processable files
5. Write back to ROM
*** Level Example
#+attr_latex: :height 17em
#+caption: First two screens of level 258
[[../../thesis/img/Level102.png]]
*** Results 1D: Sequence Prediction Only
#+attr_latex: :height 16.5em
#+caption: First two generated screens for level 258 via transformer sequence prediction only
[[../../thesis/img/Level102_gpt_1d_pred.png]]
*** Results 1D: Full Pipeline
#+caption: Complete level generated by pipeline with LSTM
[[../../thesis/img/Level0A4_lstm_1d.png]]
*** Level Example
#+attr_latex: :height 17em
#+caption: Level 260 as a different kind of “level”
[[../../thesis/img/Level104_centered.png]]
*** Results 2D: Sequence Prediction Only
#+caption: Selected screens generated for level 260 via transformer sequence prediction only
[[../../thesis/img/Level104_gpt_2d_pred.png]]
* Conclusions
*** Current Problems
- Not using one-hot-encoding per tile complicates the problem way too
  much
- Sequence predictors do not work well enough as generators (no
  overfitting)
- GANs are GANs (training is hard; interpreting losses is harder)
- Evaluating results takes too long and is annoying (would need more
  command line scripting capabilities in Lunar Magic[fn::Or we could
  roll our own...] or some objective function)
- Generating with transformers too slow
*** Future Improvements
What can be done to further improve the framework?
- General hyperparameter tuning
- NAS/random search would be amazing. \newline Idea: apply a macro to
  all models and list each parameter's value ranges. These value
  ranges may be read by a new random search module.
- Make sequence predictors more noisy and/or train them on their own
  predictions to improve sequential prediction
- More models, especially generative ones
- More modular pipeline; maybe you don't want to use a sequence
  predictor (good choice)
- More features (e.g.@@beamer:\ @@learning rate warmup)
*** My Takeaways
- Test models /in practice/ regularly
- Stacking models is way too complex for this kind of task
- Julia actually keeps all its promises (although the language and
  ecosystem are still very/too young)
- You will *never* have read enough papers on task-specific machine
  learning prior to working on it
*** Questions                                                       :B_frame:
	:PROPERTIES:
	:BEAMER_env: frame
	:BEAMER_opt: standout
	:END:
Any questions?
*** The End                                                         :B_frame:
	:PROPERTIES:
	:BEAMER_env: frame
	:BEAMER_opt: standout
	:END:
Thank you for your attention! \newline \pause There's one more thing...
* Extra Slides                                                   :B_appendix:
  :PROPERTIES:
  :BEAMER_env: appendix
  :END:
*** How Not to Fix a Bug                                            :B_frame:
	:PROPERTIES:
	:BEAMER_env: frame
	:END:
#+begin_src octave :exports code
# Given level tensor `level` and integer `x_pos` > 0
b = size(x_pos, 2)  # Size of second dim of `x_pos`
if x_pos > b  # Array bounds check (b == 1)
    return default_value
else
    return cool_heuristic(level, x_pos)
end
#+end_src
\pause
#+attr_beamer: :overlay +-
1. Function containing this snippet was never called in practice as I
   only added it to an interactive function but not the one called
   during database generation (meaning testing worked just fine)
2. After fixing 1., did not throw an error as ~size~ is defined on
   integers for broadcasting reasons. ~x_pos~ is almost always > 1
3. Result: Not fun@@beamer:\uncover<5->{; I'm learning Rust now}@@
*** The End (For Real)                                              :B_frame:
	:PROPERTIES:
	:BEAMER_env: frame
	:BEAMER_opt: standout
	:END:
Thank you for your extended attention!
